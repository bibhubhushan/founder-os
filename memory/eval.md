# Evaluation — Agent Performance Tracking

Track what works, what doesn't, which agent is best at what.

## Agent Scores (updated after each /retro)

| Agent | Tasks Done | Success Rate | Best At | Weak At |
|-------|-----------|-------------|---------|---------|
| Claude Code | 0 | — | — | — |
| Codex | 0 | — | — | — |
| AI Studio | 0 | — | — | — |
| Claude Web | 0 | — | — | — |
| Gemini | 0 | — | — | — |
| NotebookLM | 0 | — | — | — |

## Sprint Scores (updated after each /retro)

<!-- Format:
## YYYY-MM-DD
**Goal:** What we tried to do
**Result:** What actually happened
**Score:** X/10
**Best agent:** Which agent shined
**Bottleneck:** What slowed us down
**Fix:** What to change next time
-->

## Patterns Found

<!-- Things like:
- "Codex is faster than Claude Code for simple CRUD"
- "AI Studio gives better architecture answers when given more context"
- "NotebookLM is great for studying YouTube tutorials"
-->
